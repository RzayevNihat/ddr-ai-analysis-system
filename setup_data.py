"""
Data processing script - Rate Limit Safe with Checkpoints
"""

import json
import time
from pathlib import Path
from tqdm import tqdm
import logging
from collections import Counter
from datetime import datetime

from src.config import Config
from src.pdf_processor import DDRParser
from src.nlp_processor import NLPProcessor
from src.knowledge_graph import KnowledgeGraph
from src.rag_system import RAGSystem

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('setup_data.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ===== KONFIQURASIYA =====
CHECKPOINT_INTERVAL = 50  # H…ôr 50 fayldan bir checkpoint
ENABLE_CHECKPOINTS = True  # Checkpoint sistemi
# =========================

def load_checkpoint():
    """Checkpoint-d…ôn davam et"""
    checkpoint_file = Config.PROCESSED_DATA_PATH / "checkpoint.json"
    if checkpoint_file.exists():
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None

def save_checkpoint(processed_count, processed_data):
    """Checkpoint yarat"""
    checkpoint = {
        'timestamp': datetime.now().isoformat(),
        'processed_count': processed_count,
        'processed_data': processed_data
    }
    checkpoint_file = Config.PROCESSED_DATA_PATH / "checkpoint.json"
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, indent=2, ensure_ascii=False)
    logger.info(f"üíæ Checkpoint saxlandƒ±: {processed_count} fayl")

def validate_wellbore_data(data_list):
    """Quyu m…ôlumatlarƒ±nƒ±n keyfiyy…ôtini yoxlayƒ±r"""
    logger.info("\n" + "="*60)
    logger.info("QUYU M∆èLUMATLARININ VALƒ∞DASƒ∞YASI")
    logger.info("="*60)
    
    wellbores = []
    missing_wellbore = 0
    
    for d in data_list:
        wellbore = d.get('wellbore', '').strip()
        if wellbore and wellbore != '' and wellbore != 'Unknown':
            wellbores.append(wellbore)
        else:
            missing_wellbore += 1
    
    wellbore_counts = Counter(wellbores)
    
    logger.info(f"C…ômi Hesabatlar: {len(data_list)}")
    logger.info(f"Quyu adƒ± tapƒ±lan: {len(wellbores)}")
    logger.info(f"Quyu adƒ± tapƒ±lmayan: {missing_wellbore}")
    logger.info(f"Unikal Quyular: {len(wellbore_counts)}")
    
    logger.info("\nQuyu paylanmasƒ±:")
    for wb, count in wellbore_counts.most_common(20):
        logger.info(f"  {wb}: {count} hesabat")
    
    return wellbore_counts

def main():
    logger.info("="*60)
    logger.info("DDR PROSESƒ∞ BA≈ûLAYIR (Rate Limit Safe Mode)")
    logger.info("="*60)
    
    # Checkpoint yoxla
    checkpoint = load_checkpoint() if ENABLE_CHECKPOINTS else None
    start_index = 0
    processed_data = []
    
    if checkpoint:
        logger.info(f"üìÇ Checkpoint tapƒ±ldƒ±: {checkpoint['processed_count']} fayl artƒ±q emal olunub")
        response = input("Checkpoint-d…ôn davam etm…ôk ist…ôyirsiniz? (y/n): ")
        if response.lower() == 'y':
            start_index = checkpoint['processed_count']
            processed_data = checkpoint['processed_data']
            logger.info(f"‚úÖ {start_index} fayldan davam edilir")
            logger.info("‚è≥ Rate limit window t…ômizl…ônm…ôsi √º√ß√ºn 90 saniy…ô g√∂zl…ôyir...")
            time.sleep(90)
            logger.info("‚úÖ Davam edilir")
    # --- STEP 1: PARSING ---
    logger.info("\n[1/5] PDF Fayllarƒ± Oxunur (Parsing)...")
    parser = DDRParser()
    pdf_files = list(Config.PDF_DATA_PATH.glob("*.pdf"))
    
    if not pdf_files:
        logger.error(f"Fayl tapƒ±lmadƒ±: {Config.PDF_DATA_PATH}")
        return
    
    logger.info(f"{len(pdf_files)} PDF faylƒ± tapƒ±ldƒ±")
    
    # Parse (yalnƒ±z yeni fayllar)
    all_ddr_data = []
    
    if start_index > 0:
        # ∆èvv…ôlki parse m…ôlumatƒ±nƒ± y√ºkl…ô
        parsed_file = Config.PROCESSED_DATA_PATH / "parsed_ddrs.json"
        if parsed_file.exists():
            with open(parsed_file, 'r', encoding='utf-8') as f:
                all_ddr_data = json.load(f)
            logger.info(f"‚úÖ {len(all_ddr_data)} …ôvv…ôlki parse y√ºkl…ôndi")
    
    # Yeni parsing
    files_to_parse = pdf_files[len(all_ddr_data):]
    if files_to_parse:
        for pdf_path in tqdm(files_to_parse, desc="Parsing PDFs"):
            try:
                ddr_data = parser.parse_pdf(pdf_path)
                all_ddr_data.append(ddr_data)
            except Exception as e:
                logger.error(f"Parsing x…ôtasƒ± {pdf_path}: {e}")
                all_ddr_data.append({
                    'filename': pdf_path.name,
                    'error': str(e)
                })
        
        # Saxla
        parsed_file_path = Config.PROCESSED_DATA_PATH / "parsed_ddrs.json"
        with open(parsed_file_path, 'w', encoding='utf-8') as f:
            json.dump(all_ddr_data, f, indent=2, ensure_ascii=False)
    
    validate_wellbore_data(all_ddr_data)
    
# --- STEP 2: NLP PROCESSING ---
    logger.info(f"\n{'='*60}")
    logger.info(f"[2/5] NLP Processing (Rate Limit Safe)")
    logger.info(f"Ba≈ülanƒüƒ±c: {start_index}/{len(all_ddr_data)}")
    logger.info(f"Qalan: {len(all_ddr_data) - start_index} fayl")
    logger.info(f"‚è±Ô∏è  T…ôxmini vaxt: {(len(all_ddr_data) - start_index) * 15 / 60:.1f} d…ôqiq…ô")
    logger.info(f"‚ÑπÔ∏è  Rate Limit: 28 req/min, 17k tokens/min (auto-managed)")
    logger.info(f"{'='*60}\n")

    nlp_processor = NLPProcessor()
    failed_files = []

    # Progress bar
    with tqdm(total=len(all_ddr_data), initial=start_index, desc="NLP Processing") as pbar:
        for i in range(start_index, len(all_ddr_data)):
            ddr = all_ddr_data[i]
            
            if 'error' not in ddr:
                try:
                    # Process
                    params = nlp_processor.extract_parameters(ddr)
                    summary = nlp_processor.create_daily_summary(ddr)
                    events = nlp_processor.classify_events(ddr)
                    anomalies = nlp_processor.detect_anomalies(ddr)
                    
                    ddr['extracted_params'] = params
                    ddr['ai_summary'] = summary
                    ddr['classified_events'] = events
                    ddr['detected_anomalies'] = anomalies
                    
                    processed_data.append(ddr)
                    
                    # Update progress
                    pbar.set_postfix({
                        'Uƒüurlu': len(processed_data),
                        'X…ôta': len(failed_files)
                    })
                    
                    # Checkpoint
                    if ENABLE_CHECKPOINTS and (i + 1) % CHECKPOINT_INTERVAL == 0:
                        save_checkpoint(i + 1, processed_data)
                        # ƒ∞ntermediate save
                        temp_file = Config.PROCESSED_DATA_PATH / "processed_ddrs_temp.json"
                        with open(temp_file, 'w', encoding='utf-8') as f:
                            json.dump(processed_data, f, indent=2, ensure_ascii=False)
                    
                except Exception as e:
                    logger.error(f"‚ùå X…ôta: {ddr.get('filename')}: {str(e)}")
                    failed_files.append({
                        'filename': ddr.get('filename'),
                        'error': str(e)
                    })
            else:
                failed_files.append({
                    'filename': ddr.get('filename'),
                    'error': ddr.get('error')
                })
            
            pbar.update(1)
    
    # Rate limit statistikasƒ±
    stats = nlp_processor.llm.get_rate_limit_stats()
    logger.info(f"\n{'='*60}")
    logger.info("RATE LIMIT STATƒ∞STƒ∞KA")
    logger.info(f"{'='*60}")
    logger.info(f"√úmumi request: {stats['total_requests']}")
    logger.info(f"√úmumi token: {stats['total_tokens']:,}")
    logger.info(f"Rate limit hit: {stats['rate_limit_hits']}")
    logger.info(f"√úmumi g√∂zl…ôm…ô: {stats['total_wait_time']:.1f}s ({stats['total_wait_time']/60:.1f} d…ôqiq…ô)")
    
    # Summary
    logger.info(f"\n{'='*60}")
    logger.info(f"EMAL YEKUNU")
    logger.info(f"{'='*60}")
    logger.info(f"‚úÖ Uƒüurlu: {len(processed_data)}/{len(all_ddr_data)}")
    logger.info(f"‚ùå X…ôtalƒ±: {len(failed_files)}/{len(all_ddr_data)}")

    if failed_files:
        logger.warning(f"\nX…ôta ver…ôn fayllar:")
        for f in failed_files[:20]:
            logger.warning(f" - {f['filename']}: {f.get('error', 'Unknown error')[:100]}")
        
        # Save failed files list
        with open(Config.PROCESSED_DATA_PATH / "failed_files.json", 'w', encoding='utf-8') as f:
            json.dump(failed_files, f, indent=2, ensure_ascii=False)

    # Save final
    output_file = Config.PROCESSED_DATA_PATH / "processed_ddrs.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, indent=2, ensure_ascii=False)
    logger.info(f"Yekun m…ôlumat saxlanƒ±ldƒ±: {output_file}")
    
    # Checkpoint-i sil
    if ENABLE_CHECKPOINTS:
        checkpoint_file = Config.PROCESSED_DATA_PATH / "checkpoint.json"
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            logger.info("‚úÖ Checkpoint silindi (proses tamamlandƒ±)")
    
    # --- STEP 3: KNOWLEDGE GRAPH ---
    logger.info("\n[3/5] Bilik Qrafƒ± qurulur...")
    kg = KnowledgeGraph()
    for ddr in tqdm(processed_data, desc="Building KG"):
        try:
            kg.build_from_ddr(ddr)
        except Exception as e:
            logger.debug(f"KG x…ôtasƒ±: {e}")
    
    kg_stats = kg.get_statistics()
    logger.info(f"Qraf statistikasƒ±: {kg_stats}")
    kg.visualize(output_file=str(Config.PROCESSED_DATA_PATH / "knowledge_graph.html"))
    
    # --- STEP 4: RAG SYSTEM ---
    logger.info("\n[4/5] RAG Vektor Bazasƒ± qurulur...")
    try:
        rag = RAGSystem()
        rag.add_documents(processed_data)
        rag_stats = rag.get_statistics()
        logger.info(f"RAG statistikasƒ±: {rag_stats}")
    except Exception as e:
        logger.error(f"RAG x…ôtasƒ±: {e}")
    
    # --- STEP 5: TREND ANALYSIS ---
    logger.info("\n[5/5] Trend Analizi aparƒ±lƒ±r...")
    try:
        trends = nlp_processor.analyze_trends(processed_data)
        with open(Config.PROCESSED_DATA_PATH / "trends.json", 'w', encoding='utf-8') as f:
            json.dump(trends, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Trend x…ôtasƒ±: {e}")
    
    logger.info("\n" + "="*60)
    logger.info("PROSES TAMAMLANDI!")
    logger.info("="*60)
    logger.info("üöÄ T…ôtbiqi i≈ü…ô salƒ±n: streamlit run app.py")

if __name__ == "__main__":
    main()